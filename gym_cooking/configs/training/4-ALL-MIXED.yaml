# command: python main.py --config training/2
# RunInfo:
run-id: MAPPO-4x5-ALL-LAYOUTS-MIXED-RECIPES
 # for identifying in logs and in folders etc
continue-run: false # continue training from previous run
notes: Training 4x5 layouts with mixed recipes with model trained on 4x3-ALL with randomizing num_agents
tags: 4x5, eval-scalability, eval-generalization   # comma separated list of tags for this run

# Environment:
level: open-divider_salad, partial-divider_salad, full-divider_salad, block-divider_salad, cross-divider_salad, ring-divider_salad, open-divider_onion-salad, block-divider_onion-salad   # level file txt   
num-agents: 4
num-orders: 5
max-num-timesteps: 200 # quit after this many timesteps
max-num-subtasks: 14
seed: 1
device: cuda
# Environment Switches
record: false
randomize: true

# Mode:
play: false
train: true
evaluate: false
sweep: false

# Agents:
# Model type for agent (seac, ppo, bd, up, dc, fb, or greedy) must match num-agents
model1: mappo
model2: mappo
model3: mappo
model4: mappo
model1-path: pretrained/4x3/MAPPO-4x3-ALL.tar.xz
#Model files -- to load pretrained models (only works for rl and --evaluate mode)
#model1-path: # path to model file
#model2-path: 
#model3-path: 
#model4-path:

# Training Parameters:
num-processes: 100 # number of parallel environments
# MAPPO and PPO
num-total-timesteps: 6000000
num-steps-per-update: 200
# SEAC
num-episodes: 5000
num-timesteps-per-episode: 100 # Models update after every n steps

#- HyperParameters - MAPPO:
share-policy: true
use-centralized-v: true
hidden-size: 64
num-mlp-hidden-layers: 3
use-popart: true
use-valuenorm: false
use-featurenorm: true
use-naive-recurrent-policy: true
use-recurrent-policy: true
num-rnn-hidden-layers: 4
rnn-data-length: 10
lr: 0.0003
critic-lr: 0.0001
adam-eps: 0.000001
num-epoch: 10
clip-range: 0.2
batch-size: 8
entropy-coef: 0.01
value-loss-coef: 1.0
max-grad-norm: 0.5
use-gae: true
gamma: 0.99
gae-lambda: 0.95

# HyperParameters - PPO:
#batch-size: 32
#lr: 0.001
#gamma: 0.99
#gae-lambda: 0.95
#clip-range: 0.05
#entropy-coef: 0.1
#value-loss-coef: 0.1
#max-grad-norm: 0.5


# HyperParameters - SEAC (Shared Experience Actor Critic):
#lr: 0.0009
#adam-eps: 0.001
#use_gae: false
#gamma: 0.95
#gae-lambda: 0.95
#entropy-coef: 0.05
#value-loss_coef: 0.5
seac-coef: 1.0
#max-grad-norm: 0.5

# HyperParameters - Bayesian Delegation:
#### Delegation Planner
beta: 1.3
#### Navigation Planner
alpha: 0.02
tau: 2
cap: 50
main-cap: 75